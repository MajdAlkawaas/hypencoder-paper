# --- NEW CONFIGS SECTION FOR LOGGING ---
logging_config:
  # Set to "WARNING" for a quiet run, "INFO" to see setup steps.
  script_log_level: "INFO" 
  # Set this to true ONLY when you need to debug the batch creation process itself.
  log_collator: false

model_config:
  checkpoint_path: jfkback/hypencoder.6_layer
  tokenizer_pretrained_model_name_or_path: google-bert/bert-base-uncased
  query_encoder_kwargs:
    freeze_transformer: true
    model_name_or_path: google-bert/bert-base-uncased
    converter_kwargs:
      # This defines the LARGEST q-net. The hidden dim is 768.
      vector_dimensions: [768, 768, 768, 768, 768, 768, 768, 1]
      activation_type: relu
      do_residual_on_last: false
  passage_encoder_kwargs:
    freeze_transformer: true
    model_name_or_path: google-bert/bert-base-uncased
    pooling_type: cls
  shared_encoder: true

  loss_type:
    # MATRYOSHKA: define the loss type
    - matryoshka_dim_margin_mse
  loss_kwargs:
    - matryoshka_dims: [128, 768]


data_config:
  # --- CHANGE #1: Use the LOCAL file, not the Hub dataset ---
  training_huggingface_dataset: null
  training_data_jsonl: from_cached_msmarco_train_local.jsonl # <-- Point to your new local file
  training_data_split: train
  positive_filter_type: first
  num_positives_to_sample: 1
  num_negatives_to_sample: 7
# ------------------------------------------------------
    


trainer_config:
  hf_trainer_config:

    # output_dir: ./trashy
    output_dir: ./matryoshka-hyperhead-run-6-layers-atempt02
    overwrite_output_dir: true

    # --- KEY CHANGES FOR THIS EXPERIMENT ---

    # 1. Train for much longer.
    num_train_epochs: 8

    # 2. Use a more effective learning rate schedule for from-scratch training.
    learning_rate: 5e-5 # This is still a good starting point
    lr_scheduler_type: "cosine" # Change from "linear" to "cosine"
    warmup_ratio: 0.1

    # 3. Enable evaluation to find the best checkpoint.
    save_strategy: "epoch"       # Save a checkpoint at the end of each epoch
    save_total_limit: 10         # Keep the last 10 checkpoints + the best one
    
    # --- Other performance settings ---
    per_device_train_batch_size: 192
    gradient_accumulation_steps: 2
    dataloader_num_workers: 8 # Look into modifying it
    logging_steps: 300
    bf16: true
    fp16: false 