import os
from dataclasses import dataclass, field
from typing import Any, Dict, Literal, Optional

import fire
from omegaconf import OmegaConf

# Added LoggingConfig dataclass for controlling the logging behavior
@dataclass
class LoggingConfig:
    """Configuration for custom logging."""
    # Controls the overall script logging level.
    # script_log_level: str["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"] = "INFO"
    script_log_level: str = "INFO"
    # Toggles verbose logging inside the DataCollator for debugging batches
    log_collator : bool = False

@dataclass
class HypencoderModelConfig:
    tokenizer_pretrained_model_name_or_path: Optional[str] = None

    query_encoder_kwargs: Dict = field(default_factory=dict)
    passage_encoder_kwargs: Dict = field(default_factory=dict)

    # Union[str, List[str]]
    loss_type: Any = field(default_factory=lambda: [])
    # Union[Dict[str, Any], List[Dict[str, Any]]]
    loss_kwargs: Any = field(default_factory=list)

    checkpoint_path: Optional[str] = None

    model_type: str = "hypencoder"
    shared_encoder: bool = False


@dataclass
class HypencoderDataConfig:
    training_data_jsonl: Optional[str] = None
    validation_data_jsonl: Optional[str] = None

    training_huggingface_dataset: Optional[str] = None
    validation_huggingface_dataset: Optional[str] = None

    training_data_split: str = "train"
    validation_data_split: str = "train"

    positive_filter_type: str = "first"
    positive_filter_kwargs: Optional[Dict[str, Any]] = None

    label_key: Optional[str] = "score"

    num_positives_to_sample: int = 1
    num_negatives_to_sample: int = 7


@dataclass
class HFTrainerConfig:
    output_dir: str = ""
    overwrite_output_dir: bool = False
    remove_unused_columns: bool = False

    evaluation_strategy: str = "no"
    # this parameter was deactivated because it is depricated and removed from HF
    # eval_strategy: str = "no"
    eval_steps: int = 500

    per_device_train_batch_size: int = 1
    per_device_eval_batch_size: int = 1
    gradient_accumulation_steps: int = 1
    # Set to zero to avoid data loading hang issues
    dataloader_num_workers: int = 0
    # Dataloader parameters were deactivated to avoid data loading hang issues
    # This parameter was introduced in later versions of huggingface
    # dataloader_persistent_workers: bool = False
    # dataloader_prefetch_factor: Optional[int] = None
    ignore_data_skip: bool = False

    learning_rate: float = 5e-5
    weight_decay: float = 0.0

    num_train_epochs: Optional[int] = 1
    lr_scheduler_type: str = "linear"
    warmup_ratio: float = 0.05
    warmup_steps: int = 0

    logging_strategy: str = "steps"
    logging_steps: int = 1
    max_steps: int = -1

    save_strategy: str = "epoch"
    save_steps: int = 500
    save_total_limit: Optional[int] = None
    # save_only_model: bool = False
    save_safetensors: bool = True

    bf16: bool = False
    fp16: bool = False
    tf32: bool = False
    torch_compile: bool = False
    torch_compile_mode: str = "default"
    run_name: Optional[str] = None
    disable_tqdm: bool = False

    ddp_find_unused_parameters: Optional[bool] = True
    # str or bool string options are: "full_shard", "auto_wrap", ...
    fsdp: Any = False
    fsdp_config: Optional[Dict[str, Any]] = None

    report_to: str = "none"

    push_to_hub: bool = False
    hub_model_id: Optional[str] = None
    hub_strategy: str = "every_save"
    hub_private_repo: bool = True
    gradient_checkpointing: bool = False

    adam_beta1: float = 0.9
    adam_beta2: float = 0.999
    adam_epsilon: float = 1e-8


@dataclass
class HypencoderTrainerConfig:
    hf_trainer_config: HFTrainerConfig = HFTrainerConfig(
        output_dir="/tmp/output/"
    )
    resume_from_checkpoint: Optional[Any] = False


@dataclass
class HypencoderTrainingConfig:
    model_config:HypencoderModelConfig = HypencoderModelConfig(
    )
    data_config:HypencoderDataConfig = HypencoderDataConfig(
        training_data_jsonl="",
    )
    trainer_config:HypencoderTrainerConfig = HypencoderTrainerConfig(
        hf_trainer_config=HFTrainerConfig(),
    )
    # Added the logging configs
    logging_config: LoggingConfig = field(default_factory=LoggingConfig)

def relative_file_path_to_abs_path(path: str) -> str:
    return os.path.join(os.path.dirname(__file__), path)


def export_config_to_yaml(
    config_name: Optional[str] = None,
    config_dir: str = "configs",
) -> None:
    config_dir = relative_file_path_to_abs_path(config_dir)
    config = OmegaConf.structured(HypencoderTrainingConfig)

    if config_name is None:
        config_name = config.trainer_config.hf_trainer_config.run_name

    if not config_name.endswith(".yaml"):
        config_name += ".yaml"

    config_path = os.path.join(config_dir, config_name)

    if os.path.isfile(config_path):
        raise ValueError(
            f"The config file {config_path} already exists. Please choose a"
            " different config name."
        )

    print(f"Exporting config to {config_path}")
    OmegaConf.save(config=config, f=config_path)


if __name__ == "__main__":
    fire.Fire(export_config_to_yaml)
