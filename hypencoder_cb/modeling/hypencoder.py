import math
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import AutoModel, PretrainedConfig, PreTrainedModel

from hypencoder_cb.modeling.q_net import RepeatedDenseBlockConverter
from hypencoder_cb.modeling.shared import (
    BaseDualEncoder,
    BaseDualEncoderConfig,
    EncoderOutput,
)
from hypencoder_cb.modeling.similarity_and_losses import (
    HypencoderCrossEntropyLoss,
    HypencoderMarginMSELoss,
)
# MATRYOSHKA: Change, added this import
from hypencoder_cb.modeling.similarity_and_losses import HypencoderMatryoshkaDimMarginMSELoss 

def scaled_dot_product_attention(
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    dim: int,
    mask: Optional[torch.Tensor] = None,
) -> Tuple[torch.Tensor, torch.Tensor]:
    score = torch.einsum("bqd,bkd->bqk", query, key) / math.sqrt(dim)

    if mask is not None:
        score.masked_fill_(mask.unsqueeze(1) == 0, -float("Inf"))

    attention = F.softmax(score, -1)

    context = torch.einsum("bqk,bkd->bqd", [attention, value])
    return context, attention


class HypencoderConfig(PretrainedConfig):
    def __init__(
        self,
        model_name_or_path: str = "",
        freeze_transformer: bool = False,
        converter_kwargs: Dict = {},
        embedding_representation: Optional[str] = None,
        base_encoder_output_dim: int = 768,
        **kwargs,
    ):
        """
        Args:
            model_name_or_path (str, optional): The base transformer model to
                use. Defaults to "".
            freeze_transformer (bool, optional): Whether to freeze the base
                transformer model. Defaults to False.
            converter_kwargs (Dict, optional): Key-word arguments passed to the
                q-net converter function. Defaults to {}.
            embedding_representation (Optional[str], optional): If provided
                the model's output will include an embedding representations
                as well as the q-net representations. The options are "mean"
                and "cls". Defaults to None.
            base_encoder_output_dim (int, optional): The output dimension of
                the base transformer model. Defaults to 768.
        """

        super().__init__(**kwargs)
        self.model_name_or_path = model_name_or_path
        self.freeze_transformer = freeze_transformer
        self.converter_kwargs = converter_kwargs
        self.embedding_representation = embedding_representation
        self.base_encoder_output_dim = base_encoder_output_dim


@dataclass
class HypencoderOutput(EncoderOutput):
    embedding_representation: Optional[torch.Tensor] = None
    # Add fields to hold the raw generated parameters for Matryoshka training
    generated_matrices: Optional[List[torch.Tensor]] = None
    generated_vectors: Optional[List[torch.Tensor]] = None


class Hypencoder(PreTrainedModel):
    config_class = HypencoderConfig

    def __init__(self, config: HypencoderConfig) -> None:
        super(Hypencoder, self).__init__(config)
        # Loading the transformer model (e.g. bert-base-uncased)
        self.transformer = AutoModel.from_pretrained(config.model_name_or_path)
        # Creates an instances of the q-net factory
        self.weight_to_model_converter = RepeatedDenseBlockConverter(
            **config.converter_kwargs
        )
        # Freezing the transformer model parameters
        if config.freeze_transformer:
            for param in self.transformer.parameters():
                param.requires_grad = False

        # Getting the weight and bias shapes for the q-net which will
        # be generated by the hyperhead
        self.weight_shapes = self.weight_to_model_converter.weight_shapes
        self.bias_shapes = self.weight_to_model_converter.bias_shapes
        # Initializing the hyperhead
        self._initialize_hyper_head()

    def _initialize_hyper_head(self) -> None:
        # creates the learnable parameters of the hyper-head itself.
        torch.manual_seed(1)

        model_dim = self.config.base_encoder_output_dim

        self.hyper_base_matrices = nn.ParameterList(
            [
                nn.Parameter(
                    torch.zeros(1, out_dim, in_dim, requires_grad=True),
                    requires_grad=True,
                )
                for in_dim, out_dim in self.weight_shapes
            ]
        )

        self.hyper_base_vectors = nn.ParameterList(
            [
                nn.Parameter(
                    torch.zeros(out_dim, in_dim, requires_grad=True),
                    requires_grad=True,
                )
                for in_dim, out_dim in self.bias_shapes
            ]
        )

        self.weight_query_embeddings = nn.ParameterList(
            [
                nn.Parameter(
                    torch.zeros(1, out_dim, in_dim, requires_grad=True),
                    requires_grad=True,
                )
                for in_dim, out_dim in self.weight_shapes
            ]
        )

        self.bias_query_embeddings = nn.ParameterList(
            [
                nn.Parameter(
                    torch.zeros(1, out_dim, in_dim, requires_grad=True),
                    requires_grad=True,
                )
                for in_dim, out_dim in self.bias_shapes
            ]
        )

        self.weight_hyper_projection = nn.ParameterList(
            [
                nn.Linear(in_dim, in_dim)
                for in_dim, out_dim in self.weight_shapes
            ]
        )

        self.bias_hyper_projection = nn.ParameterList(
            [nn.Linear(in_dim, in_dim) for in_dim, out_dim in self.bias_shapes]
        )

        self.key_projections = nn.ParameterList(
            [
                nn.Linear(model_dim, in_dim)
                for in_dim, out_dim in (self.weight_shapes + self.bias_shapes)
            ]
        )

        self.value_projections = nn.ParameterList(
            [
                nn.Linear(model_dim, in_dim)
                for in_dim, out_dim in (self.weight_shapes + self.bias_shapes)
            ]
        )

        with torch.no_grad():
            for i in range(len(self.weight_shapes)):
                nn.init.normal_(
                    self.hyper_base_matrices[i].data, std=(2 / 768) ** 0.5
                )
                nn.init.normal_(
                    self.weight_query_embeddings[i].data, mean=0, std=10
                )
                nn.init.normal_(
                    self.weight_hyper_projection[i].weight, std=(1 / (768**2))
                )
                nn.init.zeros_(self.weight_hyper_projection[i].bias)

                # The vectors linear layer biases are not zero'd in this
                # implementation which is mostly an oversight.
                nn.init.zeros_(self.key_projections[i].bias)
                nn.init.zeros_(self.value_projections[i].bias)

            for i in range(len(self.bias_shapes)):
                nn.init.zeros_(self.hyper_base_vectors[i].data)
                nn.init.normal_(
                    self.bias_query_embeddings[i].data, mean=0, std=10
                )

    def _get_weights_and_biases(
        self, last_hidden_state: torch.Tensor, attention_mask: torch.Tensor
    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:
        """From the last hidden state of the transformer model and the
        attention mask, compute the weights and biases for the q-net.

        Args:
            last_hidden_state (torch.Tensor): Shape (bs, seq_len, hidden_size)
            attention_mask (torch.Tensor): Shape (bs, seq_len)

        Returns:
            (Tuple[List[torch.Tensor], List[torch.Tensor]]): The weights and
                biases for the q-nets. The weights are a list of tensors with
                shape (bs, out_dim, in_dim) and the biases are a list of
                tensors with shape (bs, out_dim). The order of the weights and
                biases is the same as the order of the weight and bias shapes
                provided by the converter.
        """

        batch_size = last_hidden_state.size(0)

        # Shape (num_hyper_layers, batch_size, max_seq_size, hidden_size)
        keys = [
            key_projection(last_hidden_state)
            for key_projection in self.key_projections
        ]
        values = [
            value_projection(last_hidden_state)
            for value_projection in self.value_projections
        ]

        weights = []
        # Generates the weights for the q-nets by applying scaled dot product
        # which is a form of attention.
        for i in range(len(self.weight_shapes)):
            weights.append(
                scaled_dot_product_attention(
                    query=self.weight_query_embeddings[i].repeat_interleave(
                        batch_size, dim=0
                    ),
                    key=keys[i],
                    value=values[i],
                    dim=self.weight_shapes[i][1],
                    mask=attention_mask,
                )[0]
            )

        biases = []
        offset = len(self.weight_shapes)
        # Generates the biases for the q-nets by applying scaled dot product
        # which is a form of attention.
        for i in range(len(self.bias_shapes)):
            biases.append(
                scaled_dot_product_attention(
                    query=self.bias_query_embeddings[i].repeat_interleave(
                        batch_size, dim=0
                    ),
                    key=keys[i + offset],
                    value=values[i + offset],
                    dim=self.bias_shapes[i][1],
                    mask=attention_mask,
                )[0]
            )

        weights_final = []
        biases_final = []

        for i in range(len(self.weight_shapes)):
            weights_final.append(
                self.weight_hyper_projection[i](
                    F.layer_norm(F.relu(weights[i]), weights[i].shape[2:])
                )
            )

        for i in range(len(self.bias_shapes)):
            biases_final.append(
                self.bias_hyper_projection[i](
                    F.layer_norm(F.relu(biases[i]), biases[i].shape[2:])
                )
            )

        weights_final = [
            (
                weights_final[i]
                + self.hyper_base_matrices[i].repeat(batch_size, 1, 1)
            ).transpose(2, 1)
            for i in range(len(self.weight_shapes))
        ]

        biases_final = [
            (
                biases_final[i]
                + self.hyper_base_vectors[i].repeat(batch_size, 1, 1)
            ).transpose(2, 1)
            for i in range(len(self.bias_shapes))
        ]

        return weights_final, biases_final

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor):
        # Get the contextual token representations from the transformer
        # (e.g. bert-base-uncased)
        output = self.transformer(
            input_ids=input_ids, attention_mask=attention_mask
        )

        # Shape (bs, seq_len, hidden_size)
        last_hidden_state = output.last_hidden_state

        # Calls the hyper-head to generate the q-net's parameters based
        # on the token embeddings (the transformer's output).
        # MATRYOSHKA: Generate parameters as usual
        matrices, vectors = self._get_weights_and_biases(
            last_hidden_state, attention_mask
        )

        # Calls the q-net factory to generate the q-nets.
        # Passes the matrices and vectors generated by the hyperhead to
        # the q-net factory.
        # It returns a callable NoTorchSequential object (the q-net)
        # MATRYOSHKA: No changes
        models = self.weight_to_model_converter(
            matrices, vectors, is_training=self.training
        )
        # Packages the callable q-net into a standardized output object.
        # MATRYOSHKA: Change, Return a rich output object containing everything
        output = HypencoderOutput(representation=models,
                                  generated_matrices=matrices,
                                  generated_vectors=vectors)

        # handles the optional embedding_representation if requested.
        if self.config.embedding_representation is not None:
            if self.config.embedding_representation == "mean":
                output.embedding_representation = last_hidden_state.sum(
                    dim=1
                ) / (attention_mask.sum(dim=1, keepdim=True))
            elif self.config.embedding_representation == "cls":
                output.embedding_representation = last_hidden_state[:, 0]
            else:
                raise ValueError("Unknown embedding representation type")

        return output


class TextEncoderConfig(PretrainedConfig):
    def __init__(
        self,
        model_name_or_path: str = "",
        pooling_type: str = "cls",
        freeze_transformer: bool = False,
        **kwargs,
    ):
        super().__init__(**kwargs)

        self.model_name_or_path = model_name_or_path
        self.pooling_type = pooling_type
        self.freeze_transformer = freeze_transformer


class TextEncoder(PreTrainedModel):
    # This is the document encoder which provides a single vector
    # representation for a document (it is pooled).
    # It uses CLS pooling by default or mean pooling if specified.
    config_class = TextEncoderConfig

    def __init__(self, config: TextEncoderConfig) -> None:
        super(TextEncoder, self).__init__(config)
        self.transformer = AutoModel.from_pretrained(config.model_name_or_path)
        self.pooling_type = config.pooling_type

        if config.freeze_transformer:
            for param in self.transformer.parameters():
                param.requires_grad = False

        if self.pooling_type == "mean":
            self.pool = self.mean_pool
        elif self.pooling_type == "cls":
            self.pool = self.cls_pool
    # Mean pooling of the hidden states of all non-padding tokens
    def mean_pool(self, last_hidden_state, attention_mask):
        return last_hidden_state.sum(dim=1) / attention_mask.sum(
            dim=1, keepdim=True
        )

    def cls_pool(self, last_hidden_state, attention_mask):
        return last_hidden_state[:, 0]

    def forward(self, input_ids, attention_mask):
        output = self.transformer(
            input_ids=input_ids, attention_mask=attention_mask
        )

        pooled_output = self.pool(output.last_hidden_state, attention_mask)

        return EncoderOutput(representation=pooled_output)


class HypencoderDualEncoderConfig(BaseDualEncoderConfig):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)


class HypencoderDualEncoder(BaseDualEncoder):
    # The final top-level model.
    # It encapsulates the Hypencoder (for queries) and the TextEncoder
    # (for passages) into a single, trainable module.
    # It conforms to the BaseDualEncoder interface.

    config_class = HypencoderDualEncoderConfig

    def __init__(self, config: HypencoderDualEncoderConfig):
        # super(HypencoderDualEncoder, self).__init__(config)
        super().__init__(config)

        # Creates the query encoder (Hypencoder)
        self.query_encoder = Hypencoder(
            HypencoderConfig(**config.query_encoder_kwargs)
        )

        # Creates the passage encoder (TextEncoder)
        self.passage_encoder = TextEncoder(
            TextEncoderConfig(**config.passage_encoder_kwargs)
        )

        if config.shared_encoder:
            self.passage_encoder.transformer = self.query_encoder.transformer

        # 3. NOW it is safe to call the loss setup method.
        self._get_similarity_loss(config)
        self.similarity_loss_forward_kwargs = [
            {} for _ in range(len(self.similarity_losses))
        ]
    
    # instantiates the correct loss class based on the config.
    def _get_similarity_loss(self, config: BaseDualEncoderConfig):
        self.similarity_losses = []

        for loss_type, loss_kwargs in zip(
            config.loss_type, config.loss_kwargs
        ):
            if loss_type == "margin_mse":
                self.similarity_losses.append(
                    HypencoderMarginMSELoss(**loss_kwargs)
                )
            elif loss_type == "cross_entropy":
                self.similarity_losses.append(
                    HypencoderCrossEntropyLoss(**loss_kwargs)
                )
            # MATRYOSHKA: Change, add the new Matryoshka loss type
            elif loss_type == "matryoshka_dim_margin_mse":
                # Get the converter from the query encoder
                q_net_converter = self.query_encoder.weight_to_model_converter
                # Pass it to the loss function during initialization
                self.similarity_losses.append(
                    HypencoderMatryoshkaDimMarginMSELoss(q_net_converter=q_net_converter, **loss_kwargs))
            else:
                raise ValueError(f"Unknown loss type: {loss_type}")


class TextDualEncoder(BaseDualEncoder):
    config_class = BaseDualEncoderConfig

    def __init__(self, config: BaseDualEncoderConfig):
        super(TextDualEncoder, self).__init__(config)

        self.query_encoder = TextEncoder(
            TextEncoderConfig(**config.query_encoder_kwargs)
        )

        self.passage_encoder = TextEncoder(
            TextEncoderConfig(**config.passage_encoder_kwargs)
        )

        if config.shared_encoder:
            self.passage_encoder = self.query_encoder
